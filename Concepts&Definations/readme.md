 Model-Based vs. Model-Free Reinforcement Learning
Model-Based:
The agent learns or uses a model of the environment (i.e., transition probabilities and rewards) to simulate outcomes and plan ahead.
âž¤ Example: Using a model to simulate future states before choosing an action (like MCTS or Dyna-Q).

Model-Free:
The agent learns directly from experience, without learning a model of the environment.
âž¤ Example: Q-learning, where the agent updates action values from observed rewards and transitions.

ðŸ”¶ Policy-Based vs. Value-Based Methods
Policy-Based:
Directly learns a policy (Ï€(a|s)) â€” a mapping from states to actions â€” usually using gradient methods.
âž¤ Example: REINFORCE, PPO.

Value-Based:
Learns a value function (like Q(s, a)) and derives the policy by acting greedily with respect to those values.
âž¤ Example: Q-learning, DQN.
