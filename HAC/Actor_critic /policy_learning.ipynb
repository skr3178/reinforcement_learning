{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "bThe basic framework for policy-based algorithms is straightforward. We start\n",
    "with a parameterized policy function 𝜋 𝜃 . We first (1) initialize the parameters 𝜃\n",
    "of the policy function, (2) sample a new trajectory 𝜏, (3) if 𝜏 is a good trajectory,\n",
    "increase the parameters 𝜃 towards 𝜏, otherwise decrease them, and (4) keep going\n",
    "until convergence. Algorithm 4.1 provides a framework in pseudocode. Please note\n",
    "the similarity with the codes in the previous chapter (Listing 3.1–3.3), and especially\n",
    "the deep learning algorithms, where we also optimized function parameters in a\n",
    "loop."
   ],
   "id": "be3e8762400002f1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The policy is represented by a set of parameters 𝜃 (these can be the weights in a neural network). Together, the parameters 𝜃 map the states 𝑆 to action probabilities 𝐴. When we are given a set of parameters, how should we adjust them to improve the policy? The basic idea is to randomly sample a new policy, and if it is better, adjust the parameters a bit in the direction of this new policy",
   "id": "8bf6065c4760453c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Perform gradient ascent since it is being maximized. Peform this until it converges to below a certain epsilon value",
   "id": "e42e565f94a277fc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T09:17:39.800930Z",
     "start_time": "2025-06-10T09:17:39.334721Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "gamma = 0.99"
   ],
   "id": "80c1f7e12a620e5a",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "![Alt Text](img_4.png)"
   ],
   "id": "11f5686d227fb237"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T09:31:40.122641Z",
     "start_time": "2025-06-10T09:31:40.115985Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.state_space = env.observation_space.shape[0]\n",
    "        self.action_space = env.action_space.n\n",
    "\n",
    "        self.l1 = nn.Linear(self.state_space, 128, bias=False)\n",
    "        self.l2 = nn.Linear(128, self.action_space, bias=False)\n",
    "\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # Episode policy and reward history\n",
    "        self.policy_history = torch.Tensor()\n",
    "        self.reward_episode = []\n",
    "        # Overall reward and loss history\n",
    "        self.reward_history = []\n",
    "        self.loss_history = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        model = torch.nn.Sequential(\n",
    "            self.l1,\n",
    "            nn.Dropout(p=0.6),\n",
    "            nn.ReLU(),\n",
    "            self.l2,\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        return model(x)\n",
    "\n",
    "policy = Policy()\n",
    "optimizer = optim.Adam(policy.parameters(), lr=learning_rate)"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def select_action(state):\n",
    "    # Select an action (0 or 1) by running policy model and choosing based on the probabilities in state\n",
    "    # Handle case where state is a tuple (observation, info) from newer gym versions\n",
    "    if isinstance(state, tuple):\n",
    "        state = state[0]  # Extract the observation from the tuple\n",
    "    state = torch.from_numpy(state).type(torch.FloatTensor)\n",
    "    state = policy(state)\n",
    "    c = Categorical(state)\n",
    "    action = c.sample()\n",
    "\n",
    "    # Add log probability of our chosen action to our history\n",
    "    if policy.policy_history.dim() != 0:\n",
    "        policy.policy_history = torch.cat([policy.policy_history, c.log_prob(action).unsqueeze(0)])\n",
    "    else:\n",
    "        policy.policy_history = (c.log_prob(action).unsqueeze(0))\n",
    "    return action"
   ],
   "id": "45b3908612c470b4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "![Alt Text](img_5.png)"
   ],
   "id": "22ea17290d0dc3fb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![Alt Text](img_6.png)",
   "id": "f4ee2a3a3b078df"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![Alt Text](img_7.png)",
   "id": "dea3d3cdd4a395da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def update_policy():\n",
    "    R = 0\n",
    "    rewards = []\n",
    "\n",
    "    # Discount future rewards back to the present using gamma\n",
    "    for r in policy.reward_episode[::-1]:\n",
    "        R = r + policy.gamma * R\n",
    "        rewards.insert(0, R)\n",
    "\n",
    "    # Scale rewards\n",
    "    rewards = torch.FloatTensor(rewards)\n",
    "    rewards = (rewards - rewards.mean()) / (rewards.std() + np.finfo(np.float32).eps)\n",
    "\n",
    "    # Calculate loss\n",
    "    loss = (torch.sum(torch.mul(policy.policy_history, rewards).mul(-1), -1))\n",
    "\n",
    "    # Update network weights\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Save and intialize episode history counters\n",
    "    policy.loss_history.append(loss.item())\n",
    "    policy.reward_history.append(np.sum(policy.reward_episode))\n",
    "    policy.policy_history = torch.Tensor()\n",
    "    policy.reward_episode = []"
   ],
   "id": "8578f56d6f4135ea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def main(episodes):\n",
    "    running_reward = 10\n",
    "    for episode in range(episodes):\n",
    "        # Handle new Gym API which may return (observation, info) tuple\n",
    "        reset_result = env.reset()\n",
    "        if isinstance(reset_result, tuple):\n",
    "            state = reset_result[0]  # Extract observation\n",
    "        else:\n",
    "            state = reset_result\n",
    "        done = False\n",
    "\n",
    "        for time in range(1000):\n",
    "            action = select_action(state)\n",
    "            # Step through environment using chosen action\n",
    "            # Handle new Gym API which may return (observation, reward, terminated, truncated, info)\n",
    "            step_result = env.step(action.item())\n",
    "            if len(step_result) == 4:\n",
    "                state, reward, done, _ = step_result\n",
    "            else:\n",
    "                state, reward, terminated, truncated, _ = step_result\n",
    "                done = terminated or truncated\n",
    "            # Save reward\n",
    "            policy.reward_episode.append(reward)\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # Used to determine when the environment is solved.\n",
    "        running_reward = (running_reward * 0.99) + (time * 0.01)\n",
    "\n",
    "        update_policy()\n",
    "        if episode % 50 == 0:\n",
    "            print('Episode {}\\tLast length: {:5d}\\tAverage length: {:.2f}'.format(episode, time, running_reward))\n",
    "        if running_reward > env.spec.reward_threshold:\n",
    "            print(\"Solved! Running reward is now {} and the last episode runs to {} time steps!\".format(running_reward, time))\n",
    "            break"
   ],
   "id": "b091fa85d222fee1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "episodes = 1000\n",
    "main(episodes)"
   ],
   "id": "d7cc9dc19e07d89e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
