{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "bThe basic framework for policy-based algorithms is straightforward. We start\n",
    "with a parameterized policy function ðœ‹ ðœƒ . We first (1) initialize the parameters ðœƒ\n",
    "of the policy function, (2) sample a new trajectory ðœ, (3) if ðœ is a good trajectory,\n",
    "increase the parameters ðœƒ towards ðœ, otherwise decrease them, and (4) keep going\n",
    "until convergence. Algorithm 4.1 provides a framework in pseudocode. Please note\n",
    "the similarity with the codes in the previous chapter (Listing 3.1â€“3.3), and especially\n",
    "the deep learning algorithms, where we also optimized function parameters in a\n",
    "loop."
   ],
   "id": "be3e8762400002f1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The policy is represented by a set of parameters ðœƒ (these can be the weights in a neural network). Together, the parameters ðœƒ map the states ð‘† to action probabilities ð´. When we are given a set of parameters, how should we adjust them to improve the policy? The basic idea is to randomly sample a new policy, and if it is better, adjust the parameters a bit in the direction of this new policy",
   "id": "8bf6065c4760453c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Perform gradient ascent since it is being maximized. Peform this until it converges to below a certain epsilon value",
   "id": "e42e565f94a277fc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T09:17:39.800930Z",
     "start_time": "2025-06-10T09:17:39.334721Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "gamma = 0.99"
   ],
   "id": "80c1f7e12a620e5a",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "![Alt Text](img_4.png)"
   ],
   "id": "11f5686d227fb237"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T09:31:40.122641Z",
     "start_time": "2025-06-10T09:31:40.115985Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.state_space = env.observation_space.shape[0]\n",
    "        self.action_space = env.action_space.n\n",
    "\n",
    "        self.l1 = nn.Linear(self.state_space, 128, bias=False)\n",
    "        self.l2 = nn.Linear(128, self.action_space, bias=False)\n",
    "\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # Episode policy and reward history\n",
    "        self.policy_history = torch.Tensor()\n",
    "        self.reward_episode = []\n",
    "        # Overall reward and loss history\n",
    "        self.reward_history = []\n",
    "        self.loss_history = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        model = torch.nn.Sequential(\n",
    "            self.l1,\n",
    "            nn.Dropout(p=0.6),\n",
    "            nn.ReLU(),\n",
    "            self.l2,\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        return model(x)\n",
    "\n",
    "policy = Policy()\n",
    "optimizer = optim.Adam(policy.parameters(), lr=learning_rate)"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def select_action(state):\n",
    "    # Select an action (0 or 1) by running policy model and choosing based on the probabilities in state\n",
    "    # Handle case where state is a tuple (observation, info) from newer gym versions\n",
    "    if isinstance(state, tuple):\n",
    "        state = state[0]  # Extract the observation from the tuple\n",
    "    state = torch.from_numpy(state).type(torch.FloatTensor)\n",
    "    state = policy(state)\n",
    "    c = Categorical(state)\n",
    "    action = c.sample()\n",
    "\n",
    "    # Add log probability of our chosen action to our history\n",
    "    if policy.policy_history.dim() != 0:\n",
    "        policy.policy_history = torch.cat([policy.policy_history, c.log_prob(action).unsqueeze(0)])\n",
    "    else:\n",
    "        policy.policy_history = (c.log_prob(action).unsqueeze(0))\n",
    "    return action"
   ],
   "id": "45b3908612c470b4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "![Alt Text](img_5.png)"
   ],
   "id": "22ea17290d0dc3fb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![Alt Text](img_6.png)",
   "id": "f4ee2a3a3b078df"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![Alt Text](img_7.png)",
   "id": "dea3d3cdd4a395da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def update_policy():\n",
    "    R = 0\n",
    "    rewards = []\n",
    "\n",
    "    # Discount future rewards back to the present using gamma\n",
    "    for r in policy.reward_episode[::-1]:\n",
    "        R = r + policy.gamma * R\n",
    "        rewards.insert(0, R)\n",
    "\n",
    "    # Scale rewards\n",
    "    rewards = torch.FloatTensor(rewards)\n",
    "    rewards = (rewards - rewards.mean()) / (rewards.std() + np.finfo(np.float32).eps)\n",
    "\n",
    "    # Calculate loss\n",
    "    loss = (torch.sum(torch.mul(policy.policy_history, rewards).mul(-1), -1))\n",
    "\n",
    "    # Update network weights\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Save and intialize episode history counters\n",
    "    policy.loss_history.append(loss.item())\n",
    "    policy.reward_history.append(np.sum(policy.reward_episode))\n",
    "    policy.policy_history = torch.Tensor()\n",
    "    policy.reward_episode = []"
   ],
   "id": "8578f56d6f4135ea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def main(episodes):\n",
    "    running_reward = 10\n",
    "    for episode in range(episodes):\n",
    "        # Handle new Gym API which may return (observation, info) tuple\n",
    "        reset_result = env.reset()\n",
    "        if isinstance(reset_result, tuple):\n",
    "            state = reset_result[0]  # Extract observation\n",
    "        else:\n",
    "            state = reset_result\n",
    "        done = False\n",
    "\n",
    "        for time in range(1000):\n",
    "            action = select_action(state)\n",
    "            # Step through environment using chosen action\n",
    "            # Handle new Gym API which may return (observation, reward, terminated, truncated, info)\n",
    "            step_result = env.step(action.item())\n",
    "            if len(step_result) == 4:\n",
    "                state, reward, done, _ = step_result\n",
    "            else:\n",
    "                state, reward, terminated, truncated, _ = step_result\n",
    "                done = terminated or truncated\n",
    "            # Save reward\n",
    "            policy.reward_episode.append(reward)\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # Used to determine when the environment is solved.\n",
    "        running_reward = (running_reward * 0.99) + (time * 0.01)\n",
    "\n",
    "        update_policy()\n",
    "        if episode % 50 == 0:\n",
    "            print('Episode {}\\tLast length: {:5d}\\tAverage length: {:.2f}'.format(episode, time, running_reward))\n",
    "        if running_reward > env.spec.reward_threshold:\n",
    "            print(\"Solved! Running reward is now {} and the last episode runs to {} time steps!\".format(running_reward, time))\n",
    "            break"
   ],
   "id": "b091fa85d222fee1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "episodes = 1000\n",
    "main(episodes)"
   ],
   "id": "d7cc9dc19e07d89e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
